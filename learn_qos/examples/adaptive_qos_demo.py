#!/usr/bin/env python3
"""
è´Ÿè½½æ„ŸçŸ¥è‡ªé€‚åº”é™æµç³»ç»Ÿæ¼”ç¤º

å±•ç¤ºå®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯ååŒå·¥ä½œçš„è‡ªé€‚åº”QoSç³»ç»Ÿï¼Œ
åŒ…æ‹¬è´Ÿè½½ç›‘æ§ã€åé¦ˆä¼ è¾“å’Œè‡ªé€‚åº”é™æµçš„å®Œæ•´æµç¨‹ã€‚
"""

import sys
import os
import time
import threading
import random
import matplotlib.pyplot as plt
from typing import List, Dict, Any
from collections import defaultdict
import numpy as np

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from algorithms.adaptive_qos import (
    LoadMonitor, AdaptiveRateLimiter, AIMDRateLimiter, 
    PIDRateLimiter, FeedbackGenerator, FeedbackReceiver,
    FeedbackSystem, AdaptiveStrategy, create_adaptive_limiter
)
from common.base import Packet, TrafficGenerator


class MockServer:
    """æ¨¡æ‹ŸæœåŠ¡å™¨"""
    
    def __init__(self, server_id: str, base_capacity: float = 100.0):
        self.server_id = server_id
        self.base_capacity = base_capacity
        self.load_monitor = LoadMonitor(sample_interval=0.5)
        self.feedback_generator = FeedbackGenerator(
            server_id, self.load_monitor, base_capacity
        )
        
        # æ¨¡æ‹Ÿè´Ÿè½½ç”Ÿæˆ
        self._background_load = 0.0
        self._load_thread = None
        self._stop_simulation = threading.Event()
    
    def start(self):
        """å¯åŠ¨æœåŠ¡å™¨"""
        self.load_monitor.start_monitoring()
        self._start_load_simulation()
    
    def stop(self):
        """åœæ­¢æœåŠ¡å™¨"""
        self.load_monitor.stop_monitoring()
        self._stop_load_simulation()
    
    def _start_load_simulation(self):
        """å¯åŠ¨è´Ÿè½½æ¨¡æ‹Ÿ"""
        self._stop_simulation.clear()
        self._load_thread = threading.Thread(target=self._simulate_background_load, daemon=True)
        self._load_thread.start()
    
    def _stop_load_simulation(self):
        """åœæ­¢è´Ÿè½½æ¨¡æ‹Ÿ"""
        self._stop_simulation.set()
        if self._load_thread:
            self._load_thread.join(timeout=1.0)
    
    def _simulate_background_load(self):
        """æ¨¡æ‹Ÿåå°è´Ÿè½½å˜åŒ–"""
        while not self._stop_simulation.is_set():
            # æ¨¡æ‹Ÿè´Ÿè½½æ³¢åŠ¨
            self._background_load += random.uniform(-0.1, 0.1)
            self._background_load = max(0.0, min(1.0, self._background_load))
            
            # æ¨¡æ‹Ÿä¸€äº›ç³»ç»Ÿäº‹ä»¶å¯¼è‡´çš„è´Ÿè½½çªå¢
            if random.random() < 0.05:  # 5%æ¦‚ç‡
                self._background_load = min(1.0, self._background_load + random.uniform(0.2, 0.5))
            
            time.sleep(1.0)
    
    def process_request(self, client_id: str) -> Dict[str, Any]:
        """å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚"""
        # è®°å½•è¯·æ±‚å¼€å§‹
        start_time = time.time()
        self.load_monitor.record_request_start()
        
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´ï¼ˆå—è´Ÿè½½å½±å“ï¼‰
        current_load = self.load_monitor.get_current_metrics()
        if current_load:
            load_factor = max(0.1, 1.0 - current_load.cpu_usage)
            processing_time = (1.0 / load_factor) * random.uniform(0.01, 0.05)
        else:
            processing_time = random.uniform(0.01, 0.03)
        
        # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
        time.sleep(processing_time)
        
        # è®°å½•è¯·æ±‚ç»“æŸ
        response_time_ms = (time.time() - start_time) * 1000
        is_error = random.random() < 0.02  # 2%é”™è¯¯ç‡
        self.load_monitor.record_request_end(response_time_ms, is_error)
        
        # ç”Ÿæˆåé¦ˆ
        feedback_headers = self.feedback_generator.get_feedback_for_http_headers(client_id)
        
        return {
            'status': 'error' if is_error else 'success',
            'response_time_ms': response_time_ms,
            'feedback_headers': feedback_headers,
            'data': f"Response from {self.server_id}"
        }


class MockClient:
    """æ¨¡æ‹Ÿå®¢æˆ·ç«¯"""
    
    def __init__(self, client_id: str, strategy: AdaptiveStrategy = AdaptiveStrategy.AIMD):
        self.client_id = client_id
        self.strategy = strategy
        
        # è‡ªé€‚åº”é™æµå™¨
        self.rate_limiter = create_adaptive_limiter(
            strategy,
            initial_rate=50.0,
            min_rate=1.0,
            max_rate=200.0
        )
        
        # åé¦ˆæ¥æ”¶å™¨
        self.feedback_receiver = FeedbackReceiver(client_id, feedback_timeout=5.0)
        self.feedback_receiver.add_feedback_callback(self._handle_feedback)
        
        # è¯·æ±‚ç»Ÿè®¡
        self.total_requests = 0
        self.successful_requests = 0
        self.total_response_time = 0.0
        self.rate_history = []
        self.load_level_history = []
        
        self._lock = threading.Lock()
    
    def _handle_feedback(self, feedback):
        """å¤„ç†æ¥æ”¶åˆ°çš„è´Ÿè½½åé¦ˆ"""
        # è°ƒæ•´å‘é€é€Ÿç‡
        load_info = {
            'load_level': feedback.load_level.value,
            'load_score': feedback.load_score,
            'trend': feedback.trend
        }
        
        self.rate_limiter.adjust_rate(load_info)
    
    def send_request(self, server: MockServer) -> Dict[str, Any]:
        """å‘æœåŠ¡å™¨å‘é€è¯·æ±‚"""
        with self._lock:
            self.total_requests += 1
        
        # å¤„ç†è¯·æ±‚
        response = server.process_request(self.client_id)
        
        # æ¥æ”¶åé¦ˆ
        self.feedback_receiver.receive_feedback(
            response['feedback_headers'], 
            channel='HTTP_HEADER'
        )
        
        # æ›´æ–°ç»Ÿè®¡
        with self._lock:
            if response['status'] == 'success':
                self.successful_requests += 1
                self.total_response_time += response['response_time_ms']
            
            # è®°å½•å†å²
            self.rate_history.append({
                'timestamp': time.time(),
                'rate': self.rate_limiter.get_current_rate()
            })
            
            latest_feedback = self.feedback_receiver.get_latest_feedback()
            if latest_feedback:
                self.load_level_history.append({
                    'timestamp': time.time(),
                    'load_level': latest_feedback.load_level.value,
                    'load_score': latest_feedback.load_score
                })
        
        return response
    
    def get_current_rate(self) -> float:
        """è·å–å½“å‰å‘é€é€Ÿç‡"""
        return self.rate_limiter.get_current_rate()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å®¢æˆ·ç«¯ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            success_rate = (
                self.successful_requests / self.total_requests 
                if self.total_requests > 0 else 0.0
            )
            avg_response_time = (
                self.total_response_time / self.successful_requests
                if self.successful_requests > 0 else 0.0
            )
            
            return {
                'client_id': self.client_id,
                'strategy': self.strategy.value,
                'total_requests': self.total_requests,
                'successful_requests': self.successful_requests,
                'success_rate': success_rate,
                'avg_response_time_ms': avg_response_time,
                'current_rate': self.get_current_rate(),
                'rate_adjustments': self.rate_limiter.get_adjustment_stats()
            }


def demo_single_client_server():
    """æ¼”ç¤ºå•ä¸ªå®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯çš„äº¤äº’"""
    print("ğŸ”„ å•å®¢æˆ·ç«¯-æœåŠ¡ç«¯è‡ªé€‚åº”é™æµæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæœåŠ¡ç«¯å’Œå®¢æˆ·ç«¯
    server = MockServer("server-1", base_capacity=80.0)
    client = MockClient("client-1", AdaptiveStrategy.AIMD)
    
    server.start()
    
    try:
        print(f"ğŸ–¥ï¸  æœåŠ¡ç«¯: {server.server_id} (å®¹é‡: {server.base_capacity} req/s)")
        print(f"ğŸ‘¤ å®¢æˆ·ç«¯: {client.client_id} (ç­–ç•¥: {client.strategy.value})")
        print(f"ğŸ“Š åˆå§‹å‘é€é€Ÿç‡: {client.get_current_rate():.1f} req/s")
        
        print(f"\nâ³ å¼€å§‹å‘é€è¯·æ±‚...")
        
        # æ¨¡æ‹Ÿè¯·æ±‚å‘é€
        for i in range(30):
            current_rate = client.get_current_rate()
            
            # æ ¹æ®å½“å‰é€Ÿç‡æ§åˆ¶å‘é€é—´éš”
            interval = 1.0 / max(current_rate, 1.0)
            
            response = client.send_request(server)
            
            if i % 5 == 0:  # æ¯5æ¬¡è¯·æ±‚æ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
                latest_feedback = client.feedback_receiver.get_latest_feedback()
                if latest_feedback:
                    print(f"   ç¬¬{i+1}æ¬¡è¯·æ±‚: "
                          f"é€Ÿç‡={current_rate:.1f} req/s, "
                          f"è´Ÿè½½={latest_feedback.load_level.value}, "
                          f"è¯„åˆ†={latest_feedback.load_score:.2f}, "
                          f"å“åº”æ—¶é—´={response['response_time_ms']:.1f}ms")
            
            time.sleep(interval)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
        client_stats = client.get_stats()
        for key, value in client_stats.items():
            if key != 'rate_adjustments':
                print(f"   {key}: {value}")
        
        print(f"\nğŸ”§ é€Ÿç‡è°ƒæ•´ç»Ÿè®¡:")
        rate_stats = client_stats['rate_adjustments']
        print(f"   æ€»è°ƒæ•´æ¬¡æ•°: {rate_stats['total_adjustments']}")
        print(f"   å¢åŠ æ¬¡æ•°: {rate_stats['total_increases']}")
        print(f"   å‡å°‘æ¬¡æ•°: {rate_stats['total_decreases']}")
        print(f"   æœ€ç»ˆé€Ÿç‡: {rate_stats['current_rate']:.1f} req/s")
        
    finally:
        server.stop()


def demo_multiple_strategies():
    """æ¼”ç¤ºä¸åŒè‡ªé€‚åº”ç­–ç•¥çš„æ•ˆæœå¯¹æ¯”"""
    print(f"\nğŸ†š å¤šç­–ç•¥å¯¹æ¯”æ¼”ç¤º")
    print("=" * 40)
    
    # åˆ›å»ºæœåŠ¡ç«¯
    server = MockServer("server-multi", base_capacity=100.0)
    
    # åˆ›å»ºä¸åŒç­–ç•¥çš„å®¢æˆ·ç«¯
    strategies = [
        AdaptiveStrategy.AIMD,
        AdaptiveStrategy.PID,
        AdaptiveStrategy.EXPONENTIAL_BACKOFF
    ]
    
    clients = [
        MockClient(f"client-{strategy.value.lower()}", strategy)
        for strategy in strategies
    ]
    
    server.start()
    
    try:
        print(f"ğŸ–¥ï¸  æœåŠ¡ç«¯: {server.server_id} (å®¹é‡: {server.base_capacity} req/s)")
        print(f"ğŸ‘¥ å®¢æˆ·ç«¯ç­–ç•¥: {[c.strategy.value for c in clients]}")
        
        # æ¨¡æ‹Ÿå¹¶å‘è¯·æ±‚
        def client_worker(client: MockClient, num_requests: int):
            for _ in range(num_requests):
                try:
                    current_rate = client.get_current_rate()
                    interval = 1.0 / max(current_rate, 1.0)
                    
                    client.send_request(server)
                    time.sleep(interval + random.uniform(0, 0.1))  # æ·»åŠ éšæœºæŠ–åŠ¨
                except Exception as e:
                    print(f"å®¢æˆ·ç«¯ {client.client_id} è¯·æ±‚å¤±è´¥: {e}")
        
        # å¯åŠ¨å¹¶å‘è¯·æ±‚çº¿ç¨‹
        threads = []
        for client in clients:
            thread = threading.Thread(
                target=client_worker, 
                args=(client, 20),
                daemon=True
            )
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
        print(f"\nğŸ“Š ç­–ç•¥æ•ˆæœå¯¹æ¯”:")
        print(f"{'ç­–ç•¥':<20} {'æˆåŠŸç‡':<10} {'å¹³å‡å“åº”æ—¶é—´':<15} {'æœ€ç»ˆé€Ÿç‡':<15}")
        print("-" * 60)
        
        for client in clients:
            stats = client.get_stats()
            print(f"{stats['strategy']:<20} "
                  f"{stats['success_rate']*100:.1f}%{'':<5} "
                  f"{stats['avg_response_time_ms']:.1f}ms{'':<10} "
                  f"{stats['current_rate']:.1f} req/s")
        
    finally:
        server.stop()


def demo_load_surge_handling():
    """æ¼”ç¤ºè´Ÿè½½çªå¢æƒ…å†µçš„å¤„ç†"""
    print(f"\nğŸ’¥ è´Ÿè½½çªå¢å¤„ç†æ¼”ç¤º")
    print("=" * 35)
    
    server = MockServer("server-surge", base_capacity=60.0)
    client = MockClient("client-surge", AdaptiveStrategy.AIMD)
    
    server.start()
    
    try:
        print(f"ğŸ–¥ï¸  æœåŠ¡ç«¯: {server.server_id}")
        print(f"ğŸ‘¤ å®¢æˆ·ç«¯: {client.client_id}")
        print(f"âš¡ å°†åœ¨ç¬¬10ç§’æ¨¡æ‹Ÿè´Ÿè½½çªå¢...")
        
        # è®°å½•æ•°æ®ç”¨äºå¯è§†åŒ–
        timestamps = []
        rates = []
        load_scores = []
        response_times = []
        
        start_time = time.time()
        
        for i in range(25):
            current_time = time.time() - start_time
            
            # ç¬¬10ç§’å¼€å§‹æ¨¡æ‹Ÿè´Ÿè½½çªå¢
            if 10 <= current_time < 15:
                server._background_load = 0.8  # é«˜è´Ÿè½½
                # æ¨¡æ‹Ÿæ›´å¤šçš„å¹¶å‘è¯·æ±‚å¤„ç†
                for _ in range(3):
                    server.load_monitor.record_request_start()
                    time.sleep(0.01)
                    server.load_monitor.record_request_end(100.0, False)
            
            current_rate = client.get_current_rate()
            response = client.send_request(server)
            
            # è®°å½•æ•°æ®
            timestamps.append(current_time)
            rates.append(current_rate)
            
            latest_feedback = client.feedback_receiver.get_latest_feedback()
            if latest_feedback:
                load_scores.append(latest_feedback.load_score)
            else:
                load_scores.append(0.0)
                
            response_times.append(response['response_time_ms'])
            
            if i % 5 == 0:
                print(f"   {current_time:.1f}s: é€Ÿç‡={current_rate:.1f}, "
                      f"è´Ÿè½½è¯„åˆ†={load_scores[-1]:.2f}, "
                      f"å“åº”æ—¶é—´={response['response_time_ms']:.1f}ms")
            
            # æ§åˆ¶å‘é€é—´éš”
            interval = 1.0 / max(current_rate, 1.0)
            time.sleep(interval)
        
        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        try:
            fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))
            
            # å‘é€é€Ÿç‡å˜åŒ–
            ax1.plot(timestamps, rates, 'b-', linewidth=2, label='å‘é€é€Ÿç‡')
            ax1.axvspan(10, 15, alpha=0.3, color='red', label='è´Ÿè½½çªå¢æœŸ')
            ax1.set_ylabel('é€Ÿç‡ (req/s)')
            ax1.set_title('è´Ÿè½½çªå¢æƒ…å†µä¸‹çš„è‡ªé€‚åº”é™æµæ•ˆæœ')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # è´Ÿè½½è¯„åˆ†å˜åŒ–
            ax2.plot(timestamps, load_scores, 'r-', linewidth=2, label='è´Ÿè½½è¯„åˆ†')
            ax2.axvspan(10, 15, alpha=0.3, color='red', label='è´Ÿè½½çªå¢æœŸ')
            ax2.set_ylabel('è´Ÿè½½è¯„åˆ†')
            ax2.set_ylim(0, 1)
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # å“åº”æ—¶é—´å˜åŒ–
            ax3.plot(timestamps, response_times, 'g-', linewidth=2, label='å“åº”æ—¶é—´')
            ax3.axvspan(10, 15, alpha=0.3, color='red', label='è´Ÿè½½çªå¢æœŸ')
            ax3.set_ylabel('å“åº”æ—¶é—´ (ms)')
            ax3.set_xlabel('æ—¶é—´ (ç§’)')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            output_file = '/Users/sean10/Code/Algorithm_code/learn_qos/adaptive_qos_demo.png'
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            print(f"\nğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {output_file}")
            
            plt.show()
            
        except ImportError:
            print("âŒ matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–")
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        
    finally:
        server.stop()


def demo_system_integration():
    """æ¼”ç¤ºå®Œæ•´ç³»ç»Ÿé›†æˆ"""
    print(f"\nğŸŒ å®Œæ•´ç³»ç»Ÿé›†æˆæ¼”ç¤º")
    print("=" * 35)
    
    # åˆ›å»ºåé¦ˆç³»ç»Ÿ
    feedback_system = FeedbackSystem()
    
    # åˆ›å»ºå¤šä¸ªæœåŠ¡ç«¯
    servers = [
        MockServer(f"server-{i}", base_capacity=80.0 + i*20)
        for i in range(1, 4)
    ]
    
    # åˆ›å»ºåé¦ˆç”Ÿæˆå™¨
    for server in servers:
        feedback_system.create_generator(
            server.server_id,
            server.load_monitor,
            base_capacity=server.base_capacity
        )
        server.start()
    
    # åˆ›å»ºå¤šä¸ªå®¢æˆ·ç«¯
    clients = []
    strategies = [AdaptiveStrategy.AIMD, AdaptiveStrategy.PID, AdaptiveStrategy.EXPONENTIAL_BACKOFF]
    
    for i, strategy in enumerate(strategies):
        client = MockClient(f"client-{i+1}", strategy)
        feedback_system.create_receiver(client.client_id)
        clients.append(client)
    
    try:
        print(f"ğŸ–¥ï¸  æœåŠ¡ç«¯: {[s.server_id for s in servers]}")
        print(f"ğŸ‘¥ å®¢æˆ·ç«¯: {[c.client_id for c in clients]}")
        
        # æ¨¡æ‹Ÿè´Ÿè½½å‡è¡¡åœºæ™¯
        def simulate_requests():
            for round_num in range(10):
                print(f"\n--- ç¬¬ {round_num + 1} è½®è¯·æ±‚ ---")
                
                for client in clients:
                    # éšæœºé€‰æ‹©æœåŠ¡ç«¯
                    server = random.choice(servers)
                    
                    try:
                        response = client.send_request(server)
                        
                        # ä½¿ç”¨åé¦ˆç³»ç»Ÿä¼ è¾“åé¦ˆ
                        feedback_system.simulate_feedback_exchange(
                            server.server_id,
                            client.client_id
                        )
                        
                        print(f"{client.client_id} -> {server.server_id}: "
                              f"é€Ÿç‡={client.get_current_rate():.1f}, "
                              f"å“åº”={response['response_time_ms']:.1f}ms")
                        
                    except Exception as e:
                        print(f"è¯·æ±‚å¤±è´¥: {e}")
                
                time.sleep(2)
        
        simulate_requests()
        
        # æ˜¾ç¤ºç³»ç»Ÿç»Ÿè®¡
        print(f"\nğŸ“ˆ ç³»ç»Ÿæ•´ä½“ç»Ÿè®¡:")
        system_stats = feedback_system.get_system_stats()
        
        print(f"æœåŠ¡ç«¯æ•°é‡: {system_stats['generators_count']}")
        print(f"å®¢æˆ·ç«¯æ•°é‡: {system_stats['receivers_count']}")
        
        for client in clients:
            stats = client.get_stats()
            print(f"\n{stats['client_id']} ({stats['strategy']}):")
            print(f"  æˆåŠŸç‡: {stats['success_rate']*100:.1f}%")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time_ms']:.1f}ms")
            print(f"  å½“å‰é€Ÿç‡: {stats['current_rate']:.1f} req/s")
        
    finally:
        for server in servers:
            server.stop()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è´Ÿè½½æ„ŸçŸ¥è‡ªé€‚åº”é™æµç³»ç»Ÿå®Œæ•´æ¼”ç¤º")
    print("=" * 80)
    
    try:
        # 1. å•å®¢æˆ·ç«¯æ¼”ç¤º
        demo_single_client_server()
        
        # 2. å¤šç­–ç•¥å¯¹æ¯”æ¼”ç¤º
        demo_multiple_strategies()
        
        # 3. è´Ÿè½½çªå¢å¤„ç†æ¼”ç¤º
        print(f"\n" + "=" * 80)
        response = input("æ˜¯å¦è¿è¡Œè´Ÿè½½çªå¢æ¼”ç¤º? (åŒ…å«å¯è§†åŒ–) (y/n): ").lower().strip()
        if response in ['y', 'yes', 'Y']:
            demo_load_surge_handling()
        
        # 4. ç³»ç»Ÿé›†æˆæ¼”ç¤º
        print(f"\n" + "=" * 80)
        response = input("æ˜¯å¦è¿è¡Œç³»ç»Ÿé›†æˆæ¼”ç¤º? (y/n): ").lower().strip()
        if response in ['y', 'yes', 'Y']:
            demo_system_integration()
        
        print(f"\nâœ… æ¼”ç¤ºå®Œæˆ!")
        print(f"\nğŸ¯ å…³é”®å­¦ä¹ ç‚¹:")
        print(f"   1. æœåŠ¡ç«¯å®æ—¶ç›‘æ§ç³»ç»Ÿè´Ÿè½½å¹¶ç”Ÿæˆåé¦ˆ")
        print(f"   2. å®¢æˆ·ç«¯æ ¹æ®è´Ÿè½½åé¦ˆè‡ªé€‚åº”è°ƒæ•´å‘é€é€Ÿç‡")
        print(f"   3. ä¸åŒç®—æ³•ç­–ç•¥é€‚ç”¨äºä¸åŒçš„åœºæ™¯")
        print(f"   4. ç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨åº”å¯¹è´Ÿè½½çªå¢ç­‰å¼‚å¸¸æƒ…å†µ")
        print(f"   5. æ•´ä½“ç³»ç»Ÿå®ç°äº†è´Ÿè½½å‡è¡¡å’Œæ€§èƒ½ä¼˜åŒ–")
        
        print(f"\nğŸ“š è¿›é˜¶å­¦ä¹ å»ºè®®:")
        print(f"   1. å°è¯•è°ƒæ•´ä¸åŒçš„ç®—æ³•å‚æ•°")
        print(f"   2. å®ç°æ›´å¤æ‚çš„è´Ÿè½½é¢„æµ‹ç®—æ³•")
        print(f"   3. é›†æˆåˆ°å®é™…çš„WebæœåŠ¡ä¸­")
        print(f"   4. æ·»åŠ æ›´å¤šçš„æ€§èƒ½æŒ‡æ ‡ç›‘æ§")
        
    except KeyboardInterrupt:
        print(f"\næ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")


if __name__ == "__main__":
    main()

